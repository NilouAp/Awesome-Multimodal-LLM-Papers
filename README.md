# Awesome-Multimodal-LLM-Papers
A great collection of cutting-edge studies on multi-modal LLMs

## Contents
- [Foundations of Vision-Language Pretraining](#Foundations-of-Vision-Language-Pretraining)
- [Instruction-Tuned Vision-Language Models](#Instruction-Tuned-Vision-Language-Models)

 
## Foundations of Vision-Language Pretraining 

 - [ ] BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation (ICML 2022) [[Paper]](https://proceedings.mlr.press/v162/li22n/li22n.pdf)[[Code]](https://github.com/salesforce/BLIP) 

- [ ] BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models (ICML 2023) [[Paper]](https://proceedings.mlr.press/v202/li23q/li23q.pdf)[[Code]](https://github.com/salesforce/LAVIS/tree/main/projects/blip2)


## Instruction-Tuned Vision-Language Models

- [ ] Visual Instruction Tuning (NeurIPS 2023) [[paper]](https://arxiv.org/pdf/2304.08485)[[Code]](https://github.com/haotian-liu/LLaVA) 

- [ ] Improved Baselines with Visual Instruction Tuning (CVPR 2024) [[paper]](https://openaccess.thecvf.com/content/CVPR2024/papers/Liu_Improved_Baselines_with_Visual_Instruction_Tuning_CVPR_2024_paper.pdf)[[Code]](https://github.com/haotian-liu/LLaVA)

- [ ] MINIGPT-4:ENHANCING VISION-LANGUAGE UNDERSTANDING WITH ADVANCED LARGE LANGUAGE MODELS (ICLR 2024) [[paper]](https://openreview.net/pdf?id=1tZbq88f27)[[Code]](https://github.com/Vision-CAIR/MiniGPT-4)
