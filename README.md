# Awesome-Multimodal-LLM-Papers
A great collection of cutting-edge studies on multi-modal LLMs

## Contents
- [Foundations of Vision-Language Pretraining](#Foundations-of-Vision-Language-Pretraining)
- [Instruction-Tuned Vision-Language Models](#Instruction-Tuned-Vision-Language-Models)

 
## Foundations of Vision-Language Pretraining 

 - [ ] BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation (ICML 2022) [[Paper]](https://proceedings.mlr.press/v162/li22n/li22n.pdf) [[Code]](https://github.com/salesforce/BLIP) 

- [ ] BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models (ICML 2023) [[Paper]](https://proceedings.mlr.press/v202/li23q/li23q.pdf) [[Code]](https://github.com/salesforce/LAVIS/tree/main/projects/blip2)


## Instruction-Tuned Vision-Language Models

- [ ] InstructBLIP: towards general-purpose vision-language models with instruction tuning (NeurIPS 2023) [[paper]](https://arxiv.org/pdf/2305.06500)[[Code]](https://github.com/salesforce/LAVIS/tree/main/projects/instructblip)

- [ ] Visual Instruction Tuning (NeurIPS 2023) [[paper]](https://arxiv.org/pdf/2304.08485) [[Code]](https://github.com/haotian-liu/LLaVA) 

- [ ] Improved Baselines with Visual Instruction Tuning (CVPR 2024) [[paper]](https://openaccess.thecvf.com/content/CVPR2024/papers/Liu_Improved_Baselines_with_Visual_Instruction_Tuning_CVPR_2024_paper.pdf) [[Code]](https://github.com/haotian-liu/LLaVA)

- [ ] MINIGPT-4: Enhancing Vision-language Understanding with Advanced Large Language Models (ICLR 2024) [[paper]](https://openreview.net/pdf?id=1tZbq88f27)[[Code]](https://github.com/Vision-CAIR/MiniGPT-4)

- [ ] mPLUG-Owl: Modularization Empowers Large Language Models with Multimodality [[paper]](https://arxiv.org/pdf/2304.14178) [[Code]](https://github.com/X-PLUG/mPLUG-Owl)

- [ ] mPLUG-Owl2: Revolutionizing Multi-modal Large Language Model with Modality Collaboration (CVPR 2024) [[paper]](https://openaccess.thecvf.com/content/CVPR2024/papers/Ye_mPLUG-Owl2_Revolutionizing_Multi-modal_Large_Language_Model_with_Modality_Collaboration_CVPR_2024_paper.pdf) [[Code]](https://github.com/X-PLUG/mPLUG-Owl/tree/main/mPLUG-Owl2)

- [ ] mPLUG-Owl3: Towards Long Image-Sequence Understanding in Multi-Modal Large Language Models [[paper]](https://arxiv.org/pdf/2408.04840?) [[Code]](https://github.com/X-PLUG/mPLUG-Owl)


